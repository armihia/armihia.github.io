<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MIT Technology Review Interview</title>
    <style>
        .title {
            text-align: center;
            font-weight: bolder;
            font-family: 'Times new roman';
            font-size: 40px;
            font-style:italic;
        }
        .title1 {
            text-align: center;
            font-weight: bolder;
            font-family: 'Times new roman';
            font-size: 35px;
        }
        .time {
            text-align: right;
        }
        p {
            text-indent: 2em;
            font-family: 'Times new roman';
        }
        .picture {
            text-align: center;
        }
        img {
            width: 400px;
        }
        .finally {
           text-align: right; 
        }
    </style>
</head>
<body>
    <div>
    	<div class="title">MIT Technology Review Interview</div>
        <div class="title1">Kunming University of Science and Technology Team Proposes AI-Generated Agent for Companion Robots and Online Psychological Counseling</div>
        <p class="time">2024.5.29</p>
        <hr>
    </div>
    <div>
    	<b><i>Cite from MIT Technology Review (China) https://www.mittrchina.com/</i></b>
    	<br>
    	<b><i>Original link: <a href="https://www.mittrchina.com/news/detail/13370">https://www.mittrchina.com/news/detail/13370</a></i></b>
    	<br>
    	<br>
        <p>In recent years, the development of large language models (LLMs) has made significant strides. However, they still perform poorly in tasks requiring the understanding of implicit instructions and the application of common-sense knowledge.</p>

		<p>For example, achieving human-level performance often requires multiple attempts, leading to inaccurate responses or inferences in real-world applications.</p>
		
		<p>To address these issues, a research team from Kunming University of Science and Technology recently proposed an "Internal Time-Consciousness Machine" (ITCM) based on computational consciousness structures. They developed an agent based on ITCM (ITCMA).</p>
		
		<p>This agent supports behavior generation and reasoning in an open-world context, enhancing the ability of large language models to understand implicit instructions and apply common-sense knowledge.</p>
		
		<p>The research team tested ITCMA in the Alfworld environment and found that the trained ITCMA outperformed the current best level by 9% on known datasets. Even without training, ITCMA achieved a task completion rate of 96% on known datasets, exceeding the current best level by 5%.</p>
		
		<p>These results indicate that the agent surpasses traditional agents in practicality and generalization ability.</p>
		
		<p>Notably, the team also deployed ITCMA on a quadruped robot to conduct effectiveness experiments in the real world (specific results can be seen at [https://www.bilibili.com/video/BV19t421n7Ef].</p>
		
		<p>The results showed that the untrained ITCMA achieved a task completion rate of 85%, close to its performance on unseen datasets, proving the agent's practicality in real-world environments.</p>
		<div style="text-align:center;">
		<img src="./images/report1-1.png">
		<br>
		<i>Figure | Quadruped Robot in a Real-World Environment (Source: arXiv)</i>
		</div>
		
		
		<p>From an application perspective, this agent shows promising prospects in several fields.</p>
		
		<p>Firstly, in the field of companion robots.</p>
		
		<p>Current research has demonstrated that people can develop attachment to even relatively simple digital humans. Therefore, digital humans based on generative agents could foster even stronger attachment. In this context, the agent could play a significant role in the field of companion robots.</p>
		
		<p>Secondly, in the field of virtual psychological counseling.</p>
		
		<p>In fact, due to issues such as empathy and ethics, the psychological counseling industry has been cautious about AI-provided counseling and intervention. With the advent of large language models, the industry has found that AI can at least help address some simple psychological intervention issues, thereby alleviating the pressure caused by the shortage of mental health professionals to some extent.</p>
		
		<p>Thus, digital humans based on ITCMA, incorporating emotions and mirroring, might help overcome the empathy challenges faced by existing large language models in deep interactions with humans.</p>
		
		<p>Recently, a related paper titled "ITCMA: A Generative Agent Based on a Computational Consciousness Structure" was published on the preprint platform arXiv .</p>
		
		<p>Zhang Hanzhong, a master's student at Kunming University of Science and Technology, is the first author, and Associate Professor Yin Jibin from Kunming University of Science and Technology is the corresponding author..</p>
		
		<div style="text-align:center;">
		<img src="./images/report1-2.png">
		<br>
		<i>Figure | Related Paper (Source: arXiv)</i>
		</div>
		
		<p>According to Zhang Hanzhong, the research began in 2021.</p>
		
		<div style="text-align:center;">
		<img src="./images/report1-3.png">
		<br>
		<i>Figure | Zhang Hanzhong (Source: Zhang Hanzhong)</i>
		</div>
		
		<p>At that time, the academic community generally believed that general artificial intelligence, or strong AI, was still quite distant from human capabilities. Therefore, most researchers in this field chose to focus more on weak AI, which is designed to perform specific tasks.</p>
		
		<p>Neural networks are usually considered a part of weak AI. As this "black box" algorithm continues to develop, both academia and industry have increasingly focused on its interpretability. Moreover, the effectiveness of neural networks significantly decreases when the problem domain changes.</p>
		
		<p>Against this background, the research team chose artificial psychology as the direction of their study. However, as their research deepened, the focus gradually shifted to interdisciplinary studies centered on human-computer interaction, incorporating psychology, sociology, and philosophy.</p>
		
		<p>In 2022, the emergence of ChatGPT demonstrated that models with sufficiently large parameter scales could exhibit emergent capabilities. Further, a team from Stanford University proposed generative agents centered around large language models, indicating that the intelligence emerging from these models could possess some level of sociality.</p>
		
		<p>For generative agents, the input is the current environment and past experiences, while the output is the generated behavior. The foundation for generating such behavior comes from a novel agent architecture that combines large language models with mechanisms for synthesizing and retrieving information to condition the model's output.</p>
		
		<p>Without these mechanisms, a large language model might still produce behaviors, but the resulting agent might fail to respond based on past experiences and make crucial judgments, thus lacking long-term consistency.</p>
		
		<p>For example, in the "Stanford AI Town" project, developed by researchers from Stanford University and Google, multiple agents were placed in a small town and allowed to interact freely without any intervention. The role of the agents was to enable non-player characters in the town to respond to various player behaviors.</p>
		
		<p>This research spurred numerous generative agent developments in 2023. Most of these are based on a connectionist approach, which posits that the success of large language models can reduce agents to the emergent results of complex systems.</p>
		
		<p>"In fact, we don't fully agree with this approach. Even though GPT-4 and Claude 3 have surpassed humans in many aspects, issues like memory and hallucinations still exist. Therefore, generative agents using large language models as 'brains' inevitably share these limitations," Zhang Hanzhong stated.</p>
		
		<p>From a disciplinary perspective, cognitive neuroscience differs from the neural science of biology, which underpins the brain. The former focuses less on the neurons and their connections and more on the functional parts of the brain, or "brain areas," which are interconnected yet function independently.</p>
		
		<p>The research team believes that the study of general artificial intelligence is similar. Hence, rather than focusing on constructing the underlying neural network, they opted to deconstruct the "brain" using computational consciousness structures that mimic human consciousness, dealing with simple rule sets like stream of consciousness, memory, and emotion.</p>
		
		<p>"At that time, we believed that weak AI centered solely on neural networks was unlikely to develop into general AI. Therefore, influenced by models like Integrated Information Theory and the Conscious Turing Machine, we proposed the ITCM structure by combining phenomenological models of consciousness from psychology and philosophy," Zhang Hanzhong explained.</p>
		
		<p>Based on the ITCM structure, the team developed the generative agent ITCMA, aimed at providing a framework for behavior in an open world that interacts with other agents and responds to environmental changes.</p>
		
		<p>In other words, by deconstructing ITCM through computational consciousness, researchers can create agents with "consciousness," even if this consciousness is quite rudimentary.</p>
		
		<p>"ITCMA can 'actively' do something through its own ideas and complete cross-task migration of experiences through the similarity of phenomenological fields within ITCM. It can even learn to complete tasks autonomously without any prior experience," Zhang Hanzhong said.</p>
		
		<p>Researchers have demonstrated through experiments that ITCMA can learn to find an item using tools provided by the environment, freeze it, and place it in a cabinet within a short time (within 20 steps) without any guidance.</p>
		
		<p>Evidently, the shift in research perspective allowed them to correct a series of defects in traditional generative agents caused by over-reliance on large language models.</p>
		
		<p>Zhang Hanzhong noted that the successful establishment of the ITCM model was also due to the help of a friend who majored in psychology. "When discussing how to describe the ITCM model, she provided many suggestions from a psychological perspective. She also developed a psychological theory called 'Dream Theater' and published it on Douban. This might, to some extent, become the psychological foundation for ITCM in the future," he said.</p>
		
		<p>In the current research on ITCMA, researchers have primarily studied the agent's task completion and learning capabilities as a single agent, without further investigating its collaboration and interaction effects in multi-agent scenarios. Hence, their next step is to focus on ITCMA's social coordination strategies in a social network environment.</p>
		
		<p>"In this research, many might focus more on the structure and experimental results of ITCM, but my team and I believe the most crucial aspect lies in the model that is not solely task-driven and the higher-order functional interpretation of ITCMA's consciousness," Zhang Hanzhong remarked.</p>
		
		<p>They value the differences between ITCMA and traditional agents and are committed to ensuring it possesses sufficient "subjectivity."</p>
		
		<p>Just like the philosophical definition of an "agent," they hope that the generative agent is an entity with desires, beliefs, and intentions that can take autonomous actions. "Proactivity is always the most important trait. We hope ITCMA is not just a task-driven tool but also has its own ideas and can autonomously do something," Zhang Hanzhong stated.</p>
		    </div>
    <div class="finally"></div>
</body>
</html>
